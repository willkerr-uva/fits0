{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Fitting\n",
    "This example describes a linear least-squares analysis.\n",
    "  * The code is loosely based on the example: https://root.cern/doc/master/solveLinear_8C.html\n",
    "  * Many detailed references are avilaible for the least squares technique, for example see: http://vmls-book.stanford.edu/vmls.pdf\n",
    "\n",
    "Linear regression can be used to solve for ($k$) unknown parameters ($\\theta$) in a model of the form:\n",
    "\n",
    "$\\hat f(x; \\vec\\theta) = \\theta_1*f_1(x) + \\theta_2*f_2(x) + \\theta_3*f_3(x) + ... + \\theta_k*f_k(x)$,<br>\n",
    "where the $f_i(x)$ can be arbitrary functions of $x$, but each term $\\theta_i*f_i(x)$ can only depend linearly on the parameters, $\\frac{\\partial^2 f(x)}{\\partial \\theta_{i}^2} = 0$.<br>\n",
    "For a given $x$, our predicted outcome is $\\hat y = \\hat f(x)$.\n",
    "\n",
    "Let's assume we have vectors of $n$ measurements $x_i$, then $y_i \\equiv \\hat f(x_i) = \\hat y_i$, where the equivalence is true to a good approximatation if the model is a good representation of our data.  To find the model $\\hat f$ that it is most consistent with our data, we will minimze (a function of) the residuals for each data point $r_i=y_i−\\hat y_i$.\n",
    "\n",
    "Define the vectors: \n",
    "  * $y^d= (y_1,...,y_n)$ : values of dependent variables measured in data, our observations\n",
    "  * $\\hat y^d= (\\hat y_1,...,\\hat y_n)$ : expected values of the observations, given our model\n",
    "  * $r^d= (r_1,...,r_n)$ : residuals between the data and model\n",
    "  \n",
    "For a least squares fitting model, we want to find the parameters $\\vec\\theta$ that minimize the sum of squares of the prediction error.  In performing this calculation we will weight the residuals by the inverse of the uncertainy in each measurement.  As discussed in class, the uncertainties follow a normal distribution and this is equivalent to minimizing the $\\chi^2$ value.\n",
    "\n",
    "Each measurement gives us information about the unknown parameters $\\vec \\theta$.  We can write the expectation for our model for each $x_i$ as:<br>\n",
    "$\\hat f(x_i; \\vec \\theta) = \\hat y(x_i)= A_{i1}\\theta_1 + A_{i2}\\theta_2 + ... + A_{ik}\\theta_k,~ i=1,...,n$,<br>\n",
    "where we define the $n×k$ matrix $A$ as $A_{ij}=f_j(x_i),  i= 1,...,n, ~ j= 1,...,k$.\n",
    "\n",
    "Writing this in matrix notation gives :<br>\n",
    "$\\hat y^d = {\\bf A}\\,\\vec θ$\n",
    "\n",
    "The sum of squares of the residuals is then \n",
    "$|r^d|^2 = |y^d−\\hat y^d|^2 = |y^d−{\\bf A}\\,\\vec θ|^2$, which is the quantity we want to minimze by requiring:\n",
    "$\\frac{\\partial |r^d|^2} {\\partial \\theta_i} = \\frac{\\partial  |y^d−{\\bf A}\\,\\vec θ|^2 } {\\partial \\theta_i} = 0 $.\n",
    "\n",
    "# Example: solution to linear fit for 1st order polynomial: y = a + bx\n",
    "\n",
    "Where we include the following four data points in the fit:<br>\n",
    "$x$ = {0.0,1.0,2.0,3.0}<br>\n",
    "$y^d$ = {1.4,1.5,3.7,4.1}<br>\n",
    "$\\sigma^d$ = {0.5,0.2,1.0,0.5}\n",
    "\n",
    "Using the notation above, we have \n",
    "$\\hat y^d = {\\bf A}\\,\\vec θ$, and writing out the terms explicitly gives:\n",
    "\n",
    "$\\vec θ = \\begin{pmatrix} a \\\\ b \\end{pmatrix}$, $f_1(x_i) = 1, f_2(x_i)=x_i$, eg. $y = a + bx$<br>\n",
    "$y^d = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{pmatrix}$, \n",
    "${\\bf A} =\n",
    "\\begin{pmatrix} \n",
    "A_{11} & A_{12} \\\\\n",
    "A_{21} & A_{22} \\\\ \n",
    "A_{31} & A_{32} \\\\\n",
    "A_{41} & A_{42}\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix} \n",
    "f_1(x_1) & f_2(x_1) \\\\ \n",
    "f_1(x_2) & f_2(x_2) \\\\ \n",
    "f_1(x_3) & f_2(x_3) \\\\ \n",
    "f_1(x_4) & f_2(x_4) \n",
    "\\end{pmatrix}$\n",
    "\"N_parameters\" wide, \"N_data_points\" deep.\n",
    "\n",
    "To be explicit, $\\hat y^d = {\\bf A}\\,\\vec θ$, so:\n",
    "\n",
    "$ \\begin{pmatrix} \n",
    "f_1(x_1) & f_2(x_1) \\\\ \n",
    "f_1(x_2) & f_2(x_2) \\\\ \n",
    "f_1(x_3) & f_2(x_3) \\\\ \n",
    "f_1(x_4) & f_2(x_4) \n",
    "\\end{pmatrix}$\n",
    "$\\begin{pmatrix} a \\\\ b \\end{pmatrix} = $\n",
    "$\\begin{pmatrix} \n",
    "a + bx_1 \\\\ \n",
    "a + bx_2 \\\\ \n",
    "a + bx_3 \\\\ \n",
    "a + bx_4 \\\\ \n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "\\hat y_1 \\\\ \n",
    "\\hat y_2 \\\\ \n",
    "\\hat y_3 \\\\ \n",
    "\\hat y_4 \\\\ \n",
    "\\end{pmatrix}\n",
    "=\\hat y^d $\n",
    "\n",
    "and using:\n",
    "\n",
    "$|r^d|^2 = |y^d - \\hat y^d|^2 = |y^d−{\\bf A}\\,\\vec θ|^2$, where we require: $\\frac{\\partial |r^d|^2} {\\partial \\theta_i} =0 $\n",
    "\n",
    "we have the following system of equations to solve:\n",
    "\n",
    "$ 0 = -2 \\sum_i [ y^d_i - f_1(x_i)\\theta_1 - f_2(x_i)\\theta_2 ] f_1(x_i) $ <br>\n",
    "$ 0 = -2 \\sum_i [ y^d_i - f_1(x_i)\\theta_1 - f_2(x_i)\\theta_2 ] f_2(x_i) $ <br>\n",
    "or $ 0 = (y^d - {\\bf A}\\vec\\theta){\\bf A}^T$ <br>\n",
    "For $\\chi^2$ minimization, we just multiply ${\\bf A}$ and $y^d$ by $\\frac{1}{\\sigma^d}$.  In other words $|r^d|^2=\\chi^2$.\n",
    "\n",
    "Provided the columns of ${\\bf A}$ are linearly independent, we can solve this least squares problem to find $\\vec\\theta$, the model parameter values that minimize above prediction error for our data set, using:<br>\n",
    "$\\vec\\theta = ({\\bf A}^T{\\bf A})^{-1} {\\bf A}^T y^d={\\bf A}^\\dagger y^d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses the following ROOT classes that may be new to you:\n",
    "  * TMatrixD: a matrix of doubles.  Its methods are documented in the base class [TMatrixT](https://root.cern.ch/doc/master/TMatrixT_8h.html)\n",
    "  * TVectorD: a vector of doubles.  Its methods are documented in the base class [TVectorT](https://root.cern.ch/doc/master/classTVectorT.html)\n",
    "  * [TGraphErrors](https://root.cern.ch/doc/master/classTGraphErrors.html): A version of TGraph that supports the drawing of errorbars\n",
    "\n",
    "**This notebook is written in C++ using the \"%%cpp magick\" to convert cells from interpreting python to interpreting C++.  You can always access variables from the C++ side in python using the form R.var.  See the example at the bottom of this notebook.  You will probably find working on the notebook with C++ to be awkward, see the description of the exercice below for starter code that you can run outside the notebook.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cpp -d\n",
    "#include \"Riostream.h\"\n",
    "#include \"TMatrixD.h\"\n",
    "#include \"TVectorD.h\"\n",
    "#include \"TGraphErrors.h\"\n",
    "#include \"TF1.h\"\n",
    "#include \"TH1F.h\"\n",
    "#include \"TH2F.h\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SolveLSQ returns:\n",
    "$\\vec\\theta = ({\\bf A}^T{\\bf A})^{-1} {\\bf A}^T y^d={\\bf A}^\\dagger y^d$<br>\n",
    "For the $\\chi^2$ minimization, ${\\bf A}$ and $y^d$ are assumed to be weighted by the measurement uncertinties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cpp -d\n",
    "TMatrixD SolveLSQ(const TMatrixD &A, const TMatrixD &y){\n",
    "  TMatrixD AT=(A);  \n",
    "  AT.T();            // A transpose\n",
    "  TMatrixD ATAi(AT,TMatrixD::kMult,A);\n",
    "  ATAi.Invert();\n",
    "  TMatrixD Adag(ATAi,TMatrixD::kMult,AT);  // (A^T A)^(-1) A^T\n",
    "  TMatrixD theta(Adag,TMatrixD::kMult,y);  // (A^T A)^(-1) A^T * y\n",
    "  return theta;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Perform the fit  y = a + b * x \")\n",
    "nPar  = 2;\n",
    "\n",
    "ax = np.array([0.0,1.0,2.0,3.0], dtype='d')  # x_1\n",
    "ay = np.array([1.4,1.5,3.7,4.1], dtype='d')  # y_i\n",
    "ae = np.array([0.5,0.2,1.0,0.5], dtype='d')  # sigma_i\n",
    "nPnts = len(ax)\n",
    "\n",
    "plt.errorbar(ax, ay, yerr=ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.matrix(np.zeros((nPnts, nPar)))\n",
    "#Fill the A matrix\n",
    "for nr in range(nPnts):\n",
    "    for nc in range(nPar):\n",
    "        if nc==0: A[nr,nc] = 1\n",
    "        else: A[nr,nc] = ax[nr]\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the weights to A and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nPnts):\n",
    "    A[i] = A[i] / ae[i]\n",
    "yw = (ay/ae).reshape(nPnts,1)\n",
    "A,yw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for the parameters\n",
    "\n",
    "theta =  inv(np.transpose(A).dot(A)).dot(np.transpose(A)).dot(yw)\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = np.linspace(ax.min(),ax.max())\n",
    "yi = theta[0,0] + theta[1,0]*xi\n",
    "plt.errorbar(ax, ay, yerr=ae)\n",
    "plt.plot(xi,yi,color=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back and add more data points to the data and see how the code adapts to perform your fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "\n",
    "The functions `getX` and `getY` below are used to generate a random data distribution following a function of logarithmic values.\n",
    "\n",
    "$f(x) = a + b*log(x) + c*log(x)*log(x)$\n",
    "\n",
    "The y-values are randomly generated.  You can run execute the code multiple times to see the distribution changing.\n",
    "\n",
    "Your task is to use the least squares fittng technique using the function above to extract the parameters a, b, c from the fit, based on the generated x, y, ey values.\n",
    "\n",
    "Once you have the fit working, perform the following studies:\n",
    "\n",
    " * Repeat the fit a large number of times on different data sets\n",
    " * Create a 2x2 panel canvas and plot\n",
    "   * (1--3) the distribution of values of a, b, c in three histograms\n",
    "   * (4) calculate the chi^2 for each fit and plot the distribution of the $\\chi^2$ over the experiments\n",
    " * Observe how the extracted parameters fluctuate.  How is this result affected if you increase/decrease the number of points in your experiment or increase/decrease the size of the uncertainties?\n",
    " * Compare the mean and standard deviation of the chi2 distribution to expectations\n",
    " * In a second 2x2 canvas plot\n",
    "   * (1) The values of parameter $b$ vs $a$ over the experiments (2D histogram, use the 'colz' plotting option)\n",
    "   * (2) The values of parameter $c$ vs $a$ over the experiments \n",
    "   * (3) The values of parameter $c$ vs $b$ over the experiments \n",
    "   * (4) The distribution of the *reduced* $\\chi^2$ over the experiments \n",
    "\n",
    "Place your plots and comments into a single PDF file called `LSQFit.pdf` and upload this along with your work to GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an illustration of the pseudoexperiments that are generated in the `LSQFit` program that you will modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin=1.0\n",
    "xmax=20.0\n",
    "npoints=12\n",
    "sigma=0.2\n",
    "lx=np.zeros(npoints)\n",
    "ly=np.zeros(npoints)\n",
    "ley=np.zeros(npoints)\n",
    "pars=[0.5,1.3,0.5]\n",
    "\n",
    "from math import log\n",
    "def f(x,par):\n",
    "    return par[0]+par[1]*log(x)+par[2]*log(x)*log(x)\n",
    "\n",
    "from random import gauss\n",
    "def getX(x):  # x = array-like\n",
    "    step=(xmax-xmin)/npoints\n",
    "    for i in range(npoints):\n",
    "        x[i]=xmin+i*step\n",
    "        \n",
    "def getY(x,y,ey):  # x,y,ey = array-like\n",
    "    for i in range(npoints):\n",
    "        y[i]=f(x[i],pars)+gauss(0,sigma)\n",
    "        ey[i]=sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random sampling of the (x,y) data points, rerun to generate different data sets for the plot below\n",
    "getX(lx)\n",
    "getY(lx,ly,ley)\n",
    "plt.errorbar(lx, ly, yerr=ley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
